3:I[242,[],""]
4:I[3562,[],""]
5:I[3154,["407","static/chunks/1727213d-ed770260b52092c7.js","192","static/chunks/192-63f0035328ebab3b.js","601","static/chunks/app/error-06967b165f29cafe.js"],"default"]
0:["rJXjQRmKfE3riak6AR2t7",[[["",{"children":["posts",{"children":["__PAGE__",{}]}]},"$undefined","$undefined",true],["",{"children":["posts",{"children":["__PAGE__",{},[["$L1","$L2",null],null],null]},[null,["$","$L3",null,{"parallelRouterKey":"children","segmentPath":["children","posts","children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L4",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","notFoundStyles":"$undefined"}]],null]},[[[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/99c43fdd14327af2.css","precedence":"next","crossOrigin":"$undefined"}],["$","link","1",{"rel":"stylesheet","href":"/_next/static/css/736bd5ba25105f3a.css","precedence":"next","crossOrigin":"$undefined"}]],["$","html",null,{"lang":"en","children":["$","body",null,{"children":["$","$L3",null,{"parallelRouterKey":"children","segmentPath":["children"],"error":"$5","errorStyles":[],"errorScripts":[],"template":["$","$L4",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":["$","main",null,{"children":["$","section",null,{"className":"bg-white","children":["$","div",null,{"className":"layout flex min-h-screen flex-col items-center justify-center text-center text-black","children":[["$","svg",null,{"stroke":"currentColor","fill":"currentColor","strokeWidth":"0","viewBox":"0 0 24 24","className":"drop-shadow-glow animate-flicker text-red-500","children":["$undefined",[["$","path","0",{"d":"M4.00001 20V14C4.00001 9.58172 7.58173 6 12 6C16.4183 6 20 9.58172 20 14V20H21V22H3.00001V20H4.00001ZM6.00001 14H8.00001C8.00001 11.7909 9.79087 10 12 10V8C8.6863 8 6.00001 10.6863 6.00001 14ZM11 2H13V5H11V2ZM19.7782 4.80761L21.1924 6.22183L19.0711 8.34315L17.6569 6.92893L19.7782 4.80761ZM2.80762 6.22183L4.22183 4.80761L6.34315 6.92893L4.92894 8.34315L2.80762 6.22183Z","children":[]}]]],"style":{"color":"$undefined"},"height":60,"width":60,"xmlns":"http://www.w3.org/2000/svg"}],["$","h1",null,{"className":"mt-8 text-4xl md:text-6xl","children":"Page Not Found"}],["$","a",null,{"href":"/","children":"Back to home"}]]}]}]}],"notFoundStyles":[]}]}]}]],null],null],["$L6",null]]]]
7:I[2386,["386","static/chunks/386-12980df3d0e2e1b1.js","192","static/chunks/192-63f0035328ebab3b.js","697","static/chunks/697-fdb120d005bfa27e.js","991","static/chunks/app/posts/page-60cc35b441bec9b1.js"],""]
8:I[4697,["386","static/chunks/386-12980df3d0e2e1b1.js","192","static/chunks/192-63f0035328ebab3b.js","697","static/chunks/697-fdb120d005bfa27e.js","991","static/chunks/app/posts/page-60cc35b441bec9b1.js"],"Image"]
9:I[5378,["386","static/chunks/386-12980df3d0e2e1b1.js","192","static/chunks/192-63f0035328ebab3b.js","697","static/chunks/697-fdb120d005bfa27e.js","991","static/chunks/app/posts/page-60cc35b441bec9b1.js"],"default"]
a:T1ea1,
> This post is mainly a re-formulation and detailed expansion for the section 8.4, "Direct Method" in ["14 lectures on Visual SLAM"](https://github.com/gaoxiang12/slambook-en).
> 
> When I was working on direct method SLAM, I found the notations used on the book is hard to understand and many details to derive the final equation are omitted. Hence, I wrote this post as a note and record for my own derivation of the Jacobian in Direct Method of SLAM.

<!--more-->

## Coordinate System and Symbol Table

**Assumption**

Cam1 and Cam2 have same intrinsic matrix $K$ and are pinhole camera with no distortion (or distortion is corrected for resulting image in pre-process step).

**Coordinate System**

There are two 3D coordinate systems: the camera 1 (where optic center $O_1$ is the same as world coordinate origin) and the camera 2 (with transformation of cam1 &rarr; cam2 is $T_1^2 = \xi$)

There are two 2D coordinate systems (pixel coordinates): the image 1 and image 2. I will use uv coordinate when refer to them.

**Symbol Table**

| Symbol | Domain | Shape | Meaning |
| ------ | ------ | ----- | ------- |
| $P_i$  | $\mathbb{R}^3$ | (1, 3) |i-th point in the 3-dimentional space (under cam1 coordinate) |
| $p_{1,i}$  | $\mathbb{R}^2$ | (1, 2) | i-th point's projection on cam1's sensor plane |
| $p_{2,i}$  | $\mathbb{R}^2$ | (1, 2) | i-th point's projection on cam2's sensor plane |
| $K$    | $\mathbb{R}^{3\times 3}$ | (3, 3) | Camera intrinsic matrix |
| $\xi$  | $\mathfrak{se}(3)$ | (1, 6) | Cam2's pose w.r.t. world coordinate (cam1) |
| $I_1$  | $\mathbb{R^2} \to \mathbb{R}$ | - | Illuminance map from cam1 (w/ bilinear interpolation) |
| $I_2$  | $\mathbb{R^2} \to \mathbb{R}$ | - | Illuminance map from cam2 (w/ bilinear interpolation) |
| $Exp(\cdot)$ | $\mathfrak{se}(3) \to \mathbb{R}^{4\times 4}$ | - | vector to matrix + Exponential Mapping (Lie Algebra &rarr; Transformation Matrix) |
| $Log(\cdot)$ | $\mathbb{R}^{4\times 4} \to \mathfrak{se}(3)$ | - | Logarithm Mapping + Matrix to vector (Transformation Matrix &rarr; Lie Algebra)  |
| $exp(\cdot)$ | $\mathfrak{se}(3) \to SE(3)$ | - | Exponential Mapping (Lie Algebra &rarr; Lie Group) |
| $\cdot^\wedge$ | $\mathbb{R}^{6}\to \mathbb{R}^{4\times 4}$ | - | $\mathfrak{se}(3)$ vector representation to matrix representation |
| $\tilde{\cdot}$ | - | - | Homogenous coordinate system |

*Unless explicitly specified, all the coordinates are represented under heterogeneous coordinate.*

## Step 1. Reprojection Model

According to the pinhole camera model, we have $\tilde{p_{1, i}} = KP_i$ and 
$\tilde{p_{2, i}} = K(Exp(\xi)\tilde{P_i})_{1:3}$.

By converting homogenous coordinate to heterogeneous coordinate, we can retrieve the $p_{1, i}$ and $p_{2, i}$ from $\xi$ and $K$.

## Step 2. Photometric Error as Model Residual

In direct method, we **do not** do feature point matching as this process (extracting feature point, convert to feature descriptor, run descriptor matching with or without epipolar geo constraint) is computationally heavy and error-prone.

Instead, we use the "illuminance consistency assumption". That is, between two camera frames $t$ and $t + 1$, the illuminance of a point in 3D space should be consistent. That is, 

$$
I_t(P) = I_{t + 1}(P)
$$

Naturally, the photometric error is the difference in illuminance between same point in 2 adjacent frames.

$$
e_i(\xi) = \left(I_1(p_{1, i}) - I_2(p_{2, i})\right)^2
$$

Direct SLAM make use of this assumption heavily. In the bundle adjustment process, we define the total residual of current state estimation as:

$$
\mathbf{R}(\xi) = \sum_{i = 1}^N{e_i(\xi)} = \sum_{i = 1}^N{\left(I_1(p_{1, i}) - I_2(p_{2, i})\right)^2}
$$

## Step 3. Optimization Problem Formulation

During the (local) bundle adjustment stage, we want to optimize the pose of cam2, namely $\xi \in \mathfrak{se}(3)$, to minimize the residual of state estimation. The mathematical formulation of this optimization problem is:

$$
\begin{aligned}
\xi^\star &= \arg\min_{\xi}{\mathbf{R}(\xi)}\\
    &= \arg\min_{\xi}{\sum_{i = 1}^N{e_i(\xi)}}\\
    &= \arg\min_{\xi}{\sum_{i = 1}^N{\|I_1(p_{1, i}) - I_2(p_{2, i})\|_2^2}}\\
    &= \arg\min_{\xi}{\sum_{i = 1}^N{\|I_1(KP_i) - I_2(K(Exp(\xi)\tilde{P_i})_{1:3}\|_2^2}}
\end{aligned}
$$

## Step 4. Perturbation Model and Jacobian for Direct Method

To solve this optimization problem, we can use optimizers like Gauss-Newton or Lagrange-Marquadt. These optimizers typically requires us to provide the jacobian of model. Hence, we need to derive the jacobian $\mathbf{J}$ for cam2 pose $\xi$ w.r.t. $\mathbf{R}$. Since $\mathbf{R}$ is a simple summation from $e_i(\xi)$, we will only derive $\partial e_i(\xi) / \partial \xi$ here.

To calculate the jacobian of $\mathbf{R}$ w.r.t. $\xi$, there are two main methods: the direct differentiation and perturbation model.

**Direct Differentiation** (which, we will not use)

$$
\begin{aligned}
\frac{\partial e_i(\xi)}{\partial \xi} &= \lim_{\delta\xi \to 0}\frac{e_i(\xi \oplus \delta\xi) - e_i(\xi)}{\delta\xi}\\
&= \lim_{\delta\xi\to 0}{\frac{\|I(KP_i) - I_2(K(Exp(\xi\oplus\delta\xi)\tilde{P_i})_{1:3})\|_2^2 - e_i(\xi)}{\delta\xi}}
\end{aligned}
$$

However, we have $\xi\oplus\delta\xi \neq Log(Exp(\xi)Exp(\delta\xi))$ in Lie algebra. Instead, we need to follow the [BCH formula](https://en.wikipedia.org/wiki/Baker%E2%80%93Campbell%E2%80%93Hausdorff_formula) with first-order approximation (when $\delta\xi$ is sufficiently small)

$$
Log(Exp(\delta\xi)Exp(\xi)) \approx \mathbf{J}_l(\xi)^{-1}\delta\xi + \xi
$$

However, using BCH formula requires us to calculate the $\mathbf{J}_l$, which is undesired due to its computation complexity. Hence, we will use the perturbation model to calculate the differentiation instead.

**Perturbation Model**

Comparing to the direct differentiation, where BCH formula is required, perturbation model *first* add a small perturbation on target function $e_i$, then calculate the derivative of residual w.r.t. *the perturbation term*.

That is, we will calculate $\frac{\partial e_i(\xi \oplus \delta\xi)}{\partial \delta\xi}$ instead of $\frac{\partial e_i(\xi)}{\partial \xi}$.

First, we will derive $e_i(\xi \oplus \delta\xi)$ (using left-perturbation, the result is different from right-perturbation).

$$
\begin{aligned}
e_i(\xi\oplus\delta\xi) &= I_1(KP_i) - I_2(K(Exp(\delta\xi)Exp(\xi)\tilde{P_i})_{1:3})\\
\end{aligned}
$$

The taylor expansion for $Exp(\delta \xi)$ is of form

$$
Exp(\delta\xi) = exp(\delta\xi^\wedge) = \sum_{n = 0}^\infty{\frac{1}{n!}(\delta\xi^\wedge)^n}
$$

Using a first-order taylor approximation here (should be accurate as $\delta\xi$ is a minute perturbation term), we have

$$
Exp(\delta\xi) \approx 1 + \delta\xi^\wedge
$$

Then, we have

$$
\begin{aligned}
e_i(\xi\oplus\delta\xi) &= I_1(KP_i) - I_2(K(Exp(\delta\xi)Exp(\xi)\tilde{P}_i)_{1:3})\\
    &\approx I_1(KP_i) - I_2(K((1 + \delta\xi^\wedge)Exp(\xi)\tilde{P}_i)_{1:3}) \\
    &= I_1(KP_i) - I_2(K(Exp(\xi)\tilde{P}_i)_{1:3} + K(\delta\xi^\wedge Exp(\xi)\tilde{P}_i)_{1:3})
\end{aligned}
$$

Then we apply another first-order taylor approximation

$$
\begin{aligned}
&\phantom{\approx}I_2(K(Exp(\xi)\tilde{P}_i)_{1:3} + K(\delta\xi^\wedge Exp(\xi)\tilde{P}_i)_{1:3}) \\
&\approx I_2(K(Exp(\xi)\tilde{P}_i)_{1:3}) + \frac{\partial I_2(p_2)}{\partial p_2} \frac{\partial p_2}{\partial \xi} \delta\xi
\end{aligned}
$$

And we have the fully expanded form of $e(\xi\oplus\delta\xi)$

$$
e(\xi \oplus \delta\xi) = I_1(KP_i) - \left(I_2(K(Exp(\delta\xi)Exp(\xi)P_i)_{1:3}) + \frac{\partial I_2(p_2)}{\partial p_2} \frac{\partial p_2}{\partial \xi} \delta\xi \right)
$$

where we can derive the derivative under left perturbation

$$
\frac{\partial e(\xi \oplus \delta\xi)}{\partial \delta\xi} \approx -\frac{\partial I_2(p_2)}{\partial p_2}\frac{\partial p_2}{\partial \xi}
$$
b:T20b6,
In deep learning, we usually use first-order optimizers like Stochastic Gradient Descent, Adam, or RMSProp.

However, in SLAM problems, we use Bundle Adjustment to jointly optimize camera pose and landmark coordinate in **real time**. Naive first order optimizers is not efficient enough (requires many iterations to converge) for this situation.

In this post, I will introduce the concept of second order optimizer. However, since the second order optimizer requries us to derive the Hessian matrix for the optimization target, it is usually impractical to use it directly. Therefore, we will use the Gauss-Newton and Levenberg-Marquadt optimizers instead.

<!--more-->

## Optimization Problem

The optimization problem, in general can be represented in the form of 

$$
x^* = \arg\min_{x}f(x)
$$

That is, we want to find some optimal input $x^*$ s.t. such input can minimize some given expression $f(x)$.

### Naive Optimization

When function $f$ is simple, we can find its Jacobian matrix (first order derivative) $\mathbf{J}$ and Hessian matrix (second order derivative) $\mathbf{H}$ easily.

$$
\mathbf{J} = \begin{bmatrix}
    \frac{\partial f}{\partial x_0} & \frac{\partial f} {\partial x_1} & \cdots & \frac{\partial f}{\partial x_n}
\end{bmatrix}
\quad
\mathbf{H} = \begin{bmatrix}
    \frac{\partial^2 f}{\partial x_1^2} & \frac{\partial^2f}{\partial x_1 \partial x_2} & \cdots & \frac{\partial^2 f}{\partial x_1\partial x_n} \\
    \frac{\partial^2 f}{\partial x_2\partial x_1} & \frac{\partial^2 f}{\partial x_2^2} & & \vdots \\
    \vdots & & \ddots & \vdots \\
    \frac{\partial^2 f}{\partial x_n\partial x_1} & \cdots & \cdots & \frac{\partial^2 f}{\partial x_n^2}
\end{bmatrix}
$$

We can then solve for $\mathbf{J}x = \mathbf{0}$. For every solution $x$, if the hessian matrix is positive definite, then a local minimum is found.

### Iterative Optimization

However, when the function $f$ is complex or $x$ lives in very high dimension (~10k in sparse 3D reconstruction problem, or ~10M in usual neural network models), it is practically impossible to solve for analytical solution of $\mathbf{J}$ and $\mathbf{H}$.

In this case, we need to use iterative optimization. The general algorithm for such approach can be summarized as following:

1. For some initial guess value $x_0$, we have $x \gets x_0$
2. While true
   1. Use the Taylor expansion of $f$ around $x$,
    
      If using second-order optimizer, we have
      $$
      \hat{f}(x + \Delta x) = f(x) + \mathbf{J}(x)\Delta x + \frac{1}{2}\Delta x^T\mathbf{H}(x) \Delta x
      $$
      If using first-order optimizer, we have
      $$
      \hat{f}(x + \Delta x) = f(x) + \mathbf{J}(x)\Delta x
      $$

    
    2. Solve for $\Delta x^*$ such that

      $$
      \Delta x^* = \arg\min_{\Delta x}{\hat{f}(x + \Delta x)}
      $$
    
    3. Update $x \gets x + \Delta x^*$
    4. If $\|\Delta x^*\|_2 < \varepsilon$, the solution "converges" and break out the loop
 3. Return $x$

## First Order Optimizers

When using first order optimizer, we only use the first order derivative of $f$ to calculate $\Delta x^*$. Then, we have

$$
\Delta x^* = \arg\min_{\Delta x} f(x) + \mathbf{J}(x) \Delta x = \arg\min_{\Delta x} \mathbf{J}(x) \Delta x = -\arg\max_{\Delta x}{\mathbf{J}(x) \Delta x}
$$

Obviously, the solution will be $\Delta x^* = -\mathbf{J}(x)$. This is aligned with the definition of naive Stochastic Gradient Descent (SGD).

Intuitively, we can interpret first order optimization as locally approximate the optimization target $f$ as a plane with form of $\mathbf{J}x$.

![20230723164733](https://markdown-img-1304853431.file.myqcloud.com/20230723164733.png)

> First order Taylor approximation of $\sin(x) + \cos(y) + 2$ at $(2, 2)$. *Generated by GPT-4 with code interpreter.*

Such optimizer and its variants like Adam, AdamW, RMSProp are widely used in the field of deep learning and is supported by popular libraries like PyTorch.

### Why not First Order Optimizer?

While first order optimizers can support large scale optimization problem, it generally requires more iterations to converge since linearization (first order Taylor expansion) is not a good approximation.

In applications like SLAM where bundle adjustments need to run in real-time, first order optimizer cannot fulfill our need.

Therefore, we have the second order optimizers.

## Second Order Optimizers

The second order optimizers, specifically the **Newton method** uses second order Taylor expansion as the local approximation for optimization target:

$$
\hat{f}(x + \Delta x) = f(x) + \mathbf{J}(x)\Delta x + \frac{1}{2}\Delta x^T \mathbf{H}(x) \Delta x
$$

Then we have 

$$
\Delta x^* = \arg\min_{\Delta x}{\hat{f}(x + \Delta x)} = \arg\min_{\Delta x}\mathbf{J}(x)\Delta x + \frac{1}{2}\Delta x^T\mathbf{H}\Delta x
$$

Solving

$$
\frac{\partial }{\partial\Delta x} {\left(\mathbf{J}(x)\Delta x + \frac{1}{2}\Delta x^T\mathbf{H}\Delta x\right)} = \mathbf{0}
$$

We have $\mathbf{H}(x)\Delta x^* = -\mathbf{J}(x)$.

Intuitively, the second order optimizers like Newton method is using a paraboloid to locally approximate the function surface. 

![20230723164919](https://markdown-img-1304853431.file.myqcloud.com/20230723164919.png)

> Second order Taylor approximation of $\sin(x) + \cos(y) + 2$ at $(2, 2)$. *Generated by GPT4 with code interpreter*.

### Why not Second Order Optimizer?

Second order optimizer provides a much better approximation to the original function. Hence, solver with second order optimizer can converge much faster than first order optimizer.

However, it is often not practical to calculate the $\mathbf{H}$ of $f$. If $x \in \mathbf{R}^d$, then $\mathbf{H}$ will have $d^2$ elements.

## Pseudo-second-order Optimizers

### Gauss-Newton Optimizer

Gauss-Newton optimizers requires the expression to be optimized to be in the form of sum-of-square. That is, the optimization target must have form of 

$$
\mathbf{R}(x) = \sum{f(x)^2}
$$

Then, consider the first order approximation for $f$ at $x$:

$$
\hat{f}(x + \Delta x) \approx f(x) + \mathbf{J}(x) \Delta x
$$

$$\begin{aligned}
\Delta x^* &= \arg\min_{\Delta x} \mathbf{R}(x + \Delta x)\\
  &\approx \arg\min_{\Delta x}{(f(x) + \mathbf{J}(x)\Delta x)^2}\\
  &= \arg\min_{\Delta x}{f^2(x) + 2f(x)\mathbf{J}(x)\Delta x + (\mathbf{J}(x)\Delta x)^\top (\mathbf{J}(x) \Delta x)}\\
\end{aligned}$$

Since the term in $\arg\min$ is convex (is a square), we know the optimization target must be convex. Hence, we have $\Delta x = \Delta x^*$ when $\frac{\partial (f(x) + \mathbf{J}\Delta x)^2}{\partial \Delta x} = 0$.

Hence, we have

$$
\mathbf{J}^\top(x) \mathbf{J}(x) \Delta x^* = -f(x)\mathbf{J}(x)
$$

where we can solve for $\Delta x^*$.

**Problem of Gauss-Newton Optimizer**

In production environment, we may have $\mathbf{J}$ not full-ranked. This will cause the coefficient matrix $\mathbf{J}^\top \mathbf{J}$ on the left hand side being positive **semi-definite** (not full-ranked). In this case, the equation system is in singular condition and we can't solve for $\Delta x^*$ reliably.

Also, in Gauss-Newton method, the $\Delta x^\*$ we solved for may be very large. Since we only use the first order Taylor expansion, large $\Delta x^\* $ usually indicates a poor approximation for $f(x + \Delta x^\*)$. In these cases, the residual $\mathbf{R}$ may even increase as we update $x \gets x + \Delta x$.

### Levenberg-Marquadt Optimizer

Levenberg-Marquadt optimizer is a modified version of Gauss-Newton optimizer. 

The LM optimizer use a metric $\rho$ to evaluate the quality of approximation:

$$
\rho(\Delta x) = \frac{f(x + \Delta x)}{f(x) + \mathbf{J}(x)\Delta x}
$$

When the approximation is accurate, we have $\rho(\Delta x) = 1$.

LM optimizer will then use this measure of approximation quality to control the "trusted region". The "trusted region" represents a domain where the first order approximation for $f(x)$ is acceptable. 

The update vector $\Delta x$ must be in the trusted region. Hence, the entire optimization problem is formulated as

$$
\arg\min_{\Delta x}{(f(x) + \mathbf{J}(x)\Delta x)^2} \quad \text{s.t. }D\Delta x \leq \mu
$$

where $D$ is a diagonal matrix constraining the $\Delta x$ in an ellipsoid domain.

Usually, the $D$ is configured as $diag(\mathbf{J}^\top(x)\mathbf{J}(x))$. This allows the update step to move more on the direction with lower gradient.c:T7ba,
## What is C0 Language

The programming language C0 is a carefully crafted subset of the C aimed at teaching introductory algorithms and imperative programming. It is used in **15-122 Principles of Imperative Programming** and **15-411 Compiler Design** by more than 600 students each semester.

<!--more-->

## About This Project

This project is my project for Summer Undergraduate Research Fellowship (SURF) in CMU. More importantly, it's an attempt to improve students' 15-122 learning experience.

By employing various visualization and front-end technology, we make it possible to execute and visualize C0 Language on any device with modern browser. This project also allows instructors of 15-122 to embed runnable code exerciese in Learning Material System (LMS) like Canvas or Diderot thus creating a more interactive learning environment.

## Live Demo

Try copy the following C0 code segment into the *Code Editor* then hit Compile & Run to see the result!

```c
#use <conio>

int main() {
    int*[] A = alloc_array(int*, 4);
    A[1] = alloc(int);
    *A[1] = 2;
    printf("The value @ A[1] is a pointer pointing to %d", *A[1]);
    return 0;
}
```

You can also set breakpoint on **Line 8** by clicking on the left of line number. A red dot ðŸ”´ will appear if the breakpoint is set successfully.

<iframe 
    src  ="https://cs122.andrew.cmu.edu/visualc0/" 
    width="100%" 
    style="border:1px solid silver; height:80vh; border-radius: .3rem;"
    title="C0 Visualizer Demo"
>
</iframe>

## Main Features

![Slide2](https://user-images.githubusercontent.com/47029019/188786107-2c936dd6-f0c8-4102-9e97-f93d9c37dd39.png)
![Slide3](https://user-images.githubusercontent.com/47029019/188786109-3a2f0b60-d1ed-4edd-a8c8-74effcd206e2.png)
![Slide4](https://user-images.githubusercontent.com/47029019/188786411-43c66821-0f21-434f-a270-c0f500c5f5d2.png)
![Slide5](https://user-images.githubusercontent.com/47029019/188787516-47821a85-5cd3-4394-9b8a-318138442aa0.png)
d:T1182,
This system is deployed on [My blog's file Sharing Page](/notes) as a front-end application. The source code is also avalable in `React-S3-Viewer` Repo [here](https://github.com/MarkChenYutian/React-S3-viewer).

Reference Information:

* [Viewing Photo stored in S3 Buckets](https://docs.aws.amazon.com/sdk-for-javascript/v2/developer-guide/s3-example-photos-view.html)
* [AWS SDK for JavaScript](https://docs.aws.amazon.com/sdk-for-javascript/v3/developer-guide/welcome.html)
* [AWS Cognito Identity Pool](https://docs.aws.amazon.com/sdk-for-javascript/v2/developer-guide/getting-started-browser.html#getting-started-browser-create-identity-pool)
* [SessionStorage - MDN](https://developer.mozilla.org/en-US/docs/Web/API/Window/sessionStorage)

## Access AWS Resources through SDK

![Serverless-file-system](https://markdown-img-1304853431.file.myqcloud.com/20220119171645.jpg)

A user identity pool is created using AWS Cognito. Any user authenticated / unauthenticated join this identity pool will be automatically assigned with an AWS role. Then, we create a AWS SDK key corresponding to this identity pool. Anyone access AWS Service using SDK and given key will get an role called `Cognito_MyBlogFilesUnAuth_Role`.

<!--more-->


Using IAM, we can assign this role with permission to access some specific AWS Resource. In this case, we only allow users to access the S3 storage bucket `yutian-public` and allow them to `List` and `Get` objects from the bucket.

```json
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Sid": "VisualEditor0",
            "Effect": "Allow",
            "Action": "s3:ListBucket",
            "Resource": "arn:aws:s3:::yutian-public"
        },
        {
            "Sid": "VisualEditor1",
            "Effect": "Allow",
            "Action": "s3:GetObject",
            "Resource": "arn:aws:s3:::yutian-public/*"
        }
    ]
}
```

## Local Caching

Since 2022 May, the file sharing page is reconstructed using `React` and `Ant-design`. Two major improvements are implemented in this latest version:

1. The file directory will be cached in the `SessionStorage`. For each session, the client will only request once from the AWS. This can significantly reduce the #requests.

2. When downloading, files will be cached to the `IndexDB`. Therefore, unless the user reset the IndexDB or the file is updated, every file is guaranteed to be downloaded only once on each client.

### File List Cache

![File Structure saved in Session Storage](http://markdown-img-1304853431.cosgz.myqcloud.com/20220509205039.png)

When the page is first loaded, the client will request all objects in S3 storage bucket through `ListObjectV2` API. The result is a list of objects in the bucket.

There is no file structure in S3, all the "folders" are just key of object with `/` slashes between words. Therefore, we need to parse the result to rebuild a "file structure" back from plain list.

The file structure will then be stored into `SessionStorage`. Unless the user explicitly request for reload, the `ListObjectV2` API will not be called anymore in current session.

### File Content Cache

![image-20220509211227847](http://markdown-img-1304853431.cosgz.myqcloud.com/20220509211227.png)

On the other hand, we want to reduce the amount of data downloaded from the AWS to minimize cost. Therefore, we want to cache the files that users has downloaded previously.

We will need to use `IndexedDB` since the quota for session storage is very small (~1Mb per entry). IndexedDB is much more complicated to use than SessionStorage. Luckily, there's a third-party module called `idb-key-val` that allows us to use IndexDB in an extremely simple way.

```typescript
function set(key: string, val: any)

function get(key: string): Promise<any>
```

By converting `Uint8Array` to base64 string, we can store the file in client's cache and reduce the redundant request to AWS.

<mark>Yet, there is just **one more thing**</mark>

Suppose the file on S3 is updated to a newer version but the client is still caching the older version, how can the client know when to update/descard the cache?

We resolve this problem by storing the `ETag` field along with data. `ETag` is a string that reflects the change of file content (notice it's not necessarily MD5). When the key is requested, the program will also compare the `ETag` of requested resource and current cache. If they don't match, then a later version will overwrite the current cache.e:Tb22,
## Why we need Asymptotic Notation

In most of the time, we don't need to calculate the exact computational time for a given algorithm.
For an input that is large enough, the coefficient on the lowest term will have little effect on the overall computational time for the whole algorithm. Therefore, the main trend of computational time is determined by the highest term of the polynomial.

<!--more-->

When we are focusing on how the computational time increases as the scale of input increase, we are calculating the Asymptotic Efficiency of algorithm.

What we do concern is how the running time of algorithm increase as the scale of input increase. In this case, we need to employ the asymptotic notation to help us analyze the time complexity of algorithm.

## $\Theta (g(n))$ | Big-Theta Notation
This notation represents a set of functions that has a **tight** upper bound and lower bound. If a function $f(x)$ is in the set $\Theta (g(n))$, then we know that there exists $n_0$, $c_1$, and $c_2$ such that

$$
c_1 \cdot g(n) \leq f(n) \leq c_2 \cdot g(n) \quad \forall n \geq n_0
$$


## $O(g(n))$ | Big-O Notation
This notation represents a set of functions that has a specific upper bound. If a function $f(x)$ is in the set $O(g(n))$, then we know that there exists a $n_0$ and $c$ such that

$$
0\leq f(n)\leq c\cdot g(n) \quad \forall n \geq n_0
$$

Since the big O notation only specify the upper bound of function, it is a much bigger set than big theta notation. Which means that $\Theta(n) \subseteq O(n)$.

## $\Omega(g(n))$ | Big-Omega Notation
This notation represents a set of functions that has a lower bound. For all function in the set $\Omega(g(n))$, it must satisfy that there exist $c$ and $n_0$ such that

$$
c\cdot g(n) \leq f(n) \quad \forall n \geq n_0
$$


## Amortized Analysis of Time Complexity

The **Amortized Analysis of Time Complexity** is the calculation of average time complexity of an operation.

Example: In Java, the `arrayList` item is in fact a `array`. When it is full, it will copy the elements from original array into a new array with length 1.5 times the original one.

Though it seems to be inefficient and may have a time complexity of $O(n)$ for some situation, the **Average time complexity** of adding an item into the `arrayList` is still $O(1)$.

Suppose reading & writing one element in an array will take time of $c$. Constructing an array of length $n = 1.5^m \cdot k$ will take:

$$
\begin{aligned}
T(n) &= \underbrace{2\sum_{i = 0}^{m}{(1.5)^ik\cdot c}}_{\text{Copy element across arrays}} + \underbrace{(1.5)^m kc}_{\text{Add element into last array}} \\
&= 2ck\cdot \frac{1 - 1.5^m}{1 - 1.5} + 1.5^m kc\\
&= -4ck + 4ck(1.5)^m + (1.5)^m kc\\
&= O(1) + O(n) + O(n)\\
&= O(n) 
\end{aligned}
$$

Therefore, on average, the time it takes to add one element in the array will be $O(1)$.
2:["$","div",null,{"className":"min-h-screen","children":[["$","nav",null,{"className":"sticky top-0 z-20 flex flex-row flex-nowrap justify-around items-stretch bg-white/60 backdrop-blur-[4px]","children":[["$","$L7",null,{"href":"/","className":"flex-grow text-center font-bold p-3 cursor-pointer border-b-4 hover:border-primary-400 hover:text-primary-500 animated-underline","children":"About Me"}],["$","$L7",null,{"href":"/posts","className":"flex-grow text-center font-bold p-3 cursor-pointer border-b-4 hover:border-primary-400 hover:text-primary-500 animated-underline","children":"Posts"}],["$","$L7",null,{"href":"/notes","className":"flex-grow text-center font-bold p-3 cursor-pointer border-b-4 hover:border-primary-400 hover:text-primary-500 animated-underline","children":"Notes"}]]}],["$","div",null,{"className":"px-4","children":[["$","div",null,{"className":"relative flex flex-wrap flex-row items-center justify-center w-full","children":[["$","$L8",null,{"alt":"Yutian Chen portrait","src":"/_next/static/media/Yutian2025_Squared.3fe0d7ca.jpg","width":64,"height":64,"className":"rounded-full m-4 hidden md:block"}],["$","h1",null,{"className":"text-2xl md:text-3xl font-extralight py-2","children":["Yutian Chen's ",["$","span",null,{"className":"font-semibold","children":"Posts"}]]}],["$","div",null,{"className":"flex-grow"}]]}],["$","$L9",null,{"posts":[{"filename":"direct-jacobian","title":"Direct Method SLAM - Jacobian Formulation","content":"$a","date":"$D2023-08-16T00:00:00.000Z","tags":["SLAM","Optimization"]},{"filename":"2nd-order-optim","title":"Optimizers in PyPose - Gauss Newton and Levenberg-Marquadt","content":"$b","date":"$D2023-07-14T00:00:00.000Z","tags":["Machine Learning","SLAM"]},{"filename":"c0vm","title":"C0VM.ts: C0 Visualizer on the cloud","content":"$c","date":"$D2022-12-06T00:00:00.000Z","tags":["Frontend"]},{"filename":"s3-fs","title":"Build Your Own Google Drive with AWS S3","content":"$d","date":"$D2022-01-19T00:00:00.000Z","tags":["Frontend"]},{"filename":"time-complexity","title":"Time Complexity and Asymptotic Notation","content":"$e","date":"$D2019-09-01T00:00:00.000Z","tags":["Algorithm"]}]}]]}],["$","div",null,{"children":["$","footer",null,{"className":"min-h-24 items-center justify-center py-12 text-center border-t-2 border-slate-200","children":["Â© ",2025," By"," ",["$","a",null,{"target":"_blank","rel":"noopener noreferrer","href":"https://www.yutianchen.blog/","className":"cursor-newtab animated-underline custom-link inline-flex items-center font-medium focus-visible:ring-primary-500 focus:outline-none focus-visible:rounded focus-visible:ring focus-visible:ring-offset-2 border-dark border-b border-dotted hover:border-black/0","children":"Yutian Chen"}]]}]}]]}]
6:[["$","meta","0",{"name":"viewport","content":"width=device-width, initial-scale=1"}],["$","meta","1",{"charSet":"utf-8"}],["$","title","2",{"children":"Posts | Yutian's Blog"}],["$","meta","3",{"name":"description","content":"A place for me to create and share"}],["$","link","4",{"rel":"manifest","href":"/favicon/site.webmanifest","crossOrigin":"use-credentials"}],["$","meta","5",{"name":"robots","content":"index, follow"}],["$","meta","6",{"property":"og:title","content":"Yutian's Blog"}],["$","meta","7",{"property":"og:description","content":"A place for me to create and share"}],["$","meta","8",{"property":"og:url","content":"https://www.yutianchen.blog"}],["$","meta","9",{"property":"og:site_name","content":"Yutian's Blog"}],["$","meta","10",{"property":"og:locale","content":"en_US"}],["$","meta","11",{"property":"og:image","content":"https://www.yutianchen.blog/images/og.jpg"}],["$","meta","12",{"property":"og:type","content":"website"}],["$","meta","13",{"name":"twitter:card","content":"summary_large_image"}],["$","meta","14",{"name":"twitter:title","content":"Yutian's Blog"}],["$","meta","15",{"name":"twitter:description","content":"A place for me to create and share"}],["$","meta","16",{"name":"twitter:image","content":"https://www.yutianchen.blog/images/og.jpg"}],["$","link","17",{"rel":"shortcut icon","href":"/favicon/favicon-16x16.png"}],["$","link","18",{"rel":"icon","href":"/favicon/favicon.ico"}],["$","link","19",{"rel":"apple-touch-icon","href":"/favicon/apple-touch-icon.png"}]]
1:null
