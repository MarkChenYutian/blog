---
layout: post
tags: [ NLP, Neural Network ]
category: [ Neural Network ]
title: "NLP 101: Seq2Seq 模型"
banner: "/assets/images/banners/NeuralNetworkBackground.jpg"
lang: "ch"
---

## 0 四种时间序列问题

在与时间序列相关的问题中，我们通过“时序对齐（Alignment）”与“时序同步性（Synchronous）”两个维度将所有问题分为四个不同的类型

|:---:|:---:|:---:|
|    | **对齐** | **不对齐** |
|    | <img src="https://markdown-img-1304853431.file.myqcloud.com/Screen%20Shot%202021-12-29%20at%2011.41.39%20AM.png" style="height: 100px;"/> | \
| **同步** | 输入时同时输出，一帧输入与一帧输出对应（视频标注） |  \
|     | 模型：LSTM，Naive RNN |  |
|     | <img src="https://markdown-img-1304853431.file.myqcloud.com/Screen%20Shot%202021-12-29%20at%2011.45.28%20AM.png" style="height: 100px"/>    | <img src="https://markdown-img-1304853431.file.myqcloud.com/Screen%20Shot%202021-12-29%20at%2011.43.43%20AM.png" style="height: 100px;"/> \
| **不同步** | 输入与输出的顺序相同，但是没有一一对应的关系（语音识别） | 输入一个序列，在整个输入完成后输出一个顺序不一定对应的序列 （机器翻译） \
|    | 模型：Connectionist Temporal Classification (CTC) | 模型：**Seq2Seq** |

下文提到的 Seq2Seq 模型是处理“不对齐，不同步”序列问题的一种模型。一个典型的不对齐不同步问题是机器翻译。

假如我们要让模型将英语 "I ate an apple" 翻译为德语 "Ich habe einen apfel gegessen"，我们会发现翻译结果中的语序和原来并不相同，同时，一些翻译结果并不能与英语中的词汇一一对应到。

<img src="https://markdown-img-1304853431.file.myqcloud.com/Screen%20Shot%202021-12-29%20at%2011.52.07%20AM.png" alt="Screen Shot 2021-12-29 at 11.52.07 AM" style="zoom:50%;" />

<center>Fig 1. 机器翻译输入与输出并没有简单的一一对应关系 ｜ 图片来源：CMU 11-785 Lecture 18 page 5</center>

## 1 如何确保模型获得了所需的信息？

在不对齐不同步问题中，我们不能直接使用LSTM这样的模型，因为在时间序列的 $t$ 时刻，我们并不能确定此时的模型是否已经获得了所有输出正确结果所需要的信息。因此，我们的解决方案是：**先处理完所有的输入，将所有输入编码（encode）到一个隐藏状态（Hidden State），然后再逐步对这个隐藏状态进行解码（decode）**。

通过这样的结构，我们能确定在解码过程中模型一定收到了所有所需的信息。

在运行这个模型之前，我们还有两个小问题没有解决

1. 如何让模型知道当前的输入已经结束了，可以开始解码隐藏状态了
2. 一个递归神经网络理论上可以给出无限长的输出序列，我们应该怎样获知模型已经完成了对隐藏状态的解码，并停止接受模型的输出？

实际上这两个问题的解决方案是一致的 - 我们可以在模型的词汇表中添加一个特殊的 token - `<EOS>` 这个 token 代表“End Of Sentence”。当我们向模型输入一个 `<EOS>` 时，模型就知道当前输入已经结束，并开始输出隐藏状态的解码结果。

当模型在解码阶段输出 `<EOS>` 时，我们就可以获知模型已经完成了对隐藏状态的解码。

![IMG_46AF89134238-1](https://markdown-img-1304853431.file.myqcloud.com/IMG_46AF89134238-1.jpeg)

<center>Fig 2. 一种能够确保模型在输出阶段获得了所有所需信息的模型</center>



## 2 确保解码结果的上下文相关性

上面的模型在解决了输入信息完整性的同时还有一个明显的缺陷：在解码阶段中，**每一个输出只和当前隐藏状态有关，与之前的输出没有联系**。

在现实问题中，模型输出的序列一般都会有上下文相关性 - 也就是说，知道模型在 $t - 1$ 时刻的输出会改变模型在 $t$ 时刻输出的概率分布 - $P(w_t \mid w_{t-1}) \neq P(w_t)$。比如在机器翻译的任务中，如果第 $t - 1$ 个词是 “an”，那么第 $t$ 个词是 $dark$ 的概率就应该很很低。

为了解决这个问题，我们可以将模型在 $t-1$ 时刻的输出作为模型的输入传递到 $t$ 时刻。这样带来的好处非常明显：

1. 我们可以直接在 $t$ 时刻与 $t - 1$ 时刻建立联系
2. 在解码过程中，模型在过去时刻的输出也会被编码到模型的hidden state中，所以模型可以“知道”自己在整个解码阶段中曾经输出过什么内容。

![IMG_C2214BB4110E-1](https://markdown-img-1304853431.file.myqcloud.com/IMG_C2214BB4110E-1.jpeg)

<center>Fig 3. 一种能够考虑到解码结果上下文相关性的 Encoder-Decoder 模型</center>

在图三这种模型中，接受输入的部分被称为**编码器（Encoder）**。编码器会“总结”输入内容并将结果总结到自己的隐藏状态中。当编码器接受完整个输入后，其隐藏状态被称为 “Summary Vector”，因为这个张量包含了整个输入的语义。

接着，Summary Vector 被传递到了**解码器（Decoder）**的隐藏状态。解码器会解析传入的隐藏状态并（在这个例子中）给出翻译结果。

这样一种由 Encoder 和 Decoder 构成的模型就是我们所说的 Seq2Seq 模型了。

## 3 Seq2Seq 模型与词嵌入 - 我们如何解读解码结果？

> Partially Adapted from 11-785 Lec. 18 Notes P45 - P56

>  [词嵌入 (Word Embedding)]({{site.baseurl}}/2021/Word-Embedding.html) 是一种将高维度的，使用 One-hot 编码的词汇表嵌入到低维空间的方法。这里泛指用向量表示自然语言。

解码器在运行时会输出向量，我们可以通过“查表”的方法将自然语言转化为向量，但是我们如何去解读解码器输出的结果来将向量转化回自然语言呢？

*为了方便起见，我们在下文中先假设所有词汇都是通过 one-hot 方式编码的。同时，我们使用 $y_t^{A}$ 描述在时刻模型在时刻 $t$ 输出的向量中表述词汇为 $A$ 的概率*。

任意一种解读解码结果的目标都是一样的 - 对于解码器给出的一串向量输出 $y_1, \cdots, y_t$，我们要从中找到对应的词汇 $O_1, \cdots, O_n$ 使得 $P(O_1, \cdots, O_n \mid x_1,\cdots, x_m)$ 最大化。也就是说，我们希望解读方法能够做到下面这样的效果：

$$
\text{argmax}_{O_1, \cdots, O_n}(y_1^{O_1}\cdots y_n^{O_n})
$$

### 3.1 贪心算法解读解码结果

一种简单的思路是使用**贪心算法** - 在每一时刻 $t$，我们都选择当前输出向量 $y_t$ 所代表的，（相似度）概率最高的词向量 $O$ 作为 $O_t$ 的解读结果。

$$
\DeclareMathOperator*{\argmax}{argmax}
O_t = \argmax_{O_t}(y_t^{O_t})
$$

然而，这样的方法有一个弊端：如果在时刻 $t$ 模型的最优输出并不是正确的结果，从 $t + 1$ 开始的每个时刻，模型都会受到这样一个错误输出的影响，最后导致模型“越走越偏”。

![](https://markdown-img-1304853431.file.myqcloud.com/IMG_917415594339-1.jpeg)

<center>Fig 4.  直接使用贪心算法对解码器输出进行解读可能导致未来的序列被“误导”</center>

### 3.2 既然不够信息做选择，我全都要！

如果因为贪心算法存在上一段描述的缺陷就拒绝使用贪心算法，我们会遇到一个问题：**在 $t$ 时刻做出错误的选择一定会导致整个序列的错误与偏差，但是我们在 $t$ 时刻并不知道哪个选择是错误的**。那么我们怎么解决这个问题呢？

>  实际上，我们的大脑每时每刻都在处理同样的问题 - 自然语言中充斥着模糊性，这种模糊性来自于词法(Lexical Ambiguity)，结构(Structural Ambiguity) 和 语音(Acoustic Ambiguity) 。而大脑在日常交流中在无时无刻的进行去模糊化的动作。用于去模糊化的信息主要来自于对话的上下文和场景。
>
> 也就是说 - 在日常交流的过程中我们的大脑会先保留时刻 $t$ 的语言模糊性，等到积累了足够的上下文后再对 $t$ 时刻的结果进行去模糊化操作。

**也许相似的过程也能用在我们对 Seq2Seq 模型的输出解读上！**

既然我们在时刻 $t$ 无法得知足够的信息来做出选择，那么我们就*不做出选择* - 直到我们积累了足够多的信息来做出选择我们再返回来 $t$ 时刻进行决择。在每个时刻，我们选择**概率最高的$n$个词向量**作为对解码器在 $t$ 时刻输出的解读。我们将解码器 “分叉到不同的时间线”上 - 在每个分叉中，我们使用不同的词向量作为 $t + 1$ 时刻的解码器输入。

![IMG_3B1063339A5F-1](https://markdown-img-1304853431.file.myqcloud.com/IMG_3B1063339A5F-1.jpeg)

<center>Fig 5. 每一个时刻，如果概率最高的两个解读结果概率差距不大，我们可以将解码器“分叉”，在不同的assumption 上继续推理并在未来回溯至该时刻进行选择</center>

当模型在某个分支上输出了 `<EOS>` 这个特殊的token时，这个分支就结束了。因为 `<EOS>` 是约定的结束解码器输出序列的标志符。

<img src="https://markdown-img-1304853431.file.myqcloud.com/IMG_007491957EB3-1.jpeg" alt="IMG_007491957EB3-1" style="zoom: 25%;" />

<center>Fig 6. 解码器每个时刻因为对解码结果的不同解读产生不同的分叉。每个分叉在输出<code>&lt;EOS&gt;</code>时结束</center>

当解码器所有的分支都结束输出以后，我们可以对每一个分支输出的序列进行评估并在其中选择最优的序列。

> 因为分支的数量随序列长度成指数上升，这里我们一般会在评估时使用一种叫 [Beam Search](https://en.wikipedia.org/wiki/Beam_search) 的启发式搜索算法对需要评估的分支进行剪枝。

## 4 Seq2Seq 模型的训练

在训练 Seq2Seq 模型时，我们不用对模型在编码阶段的输出做任何评估。我们只需要对比模型在解码阶段给出的输出即可。

在训练阶段，我们并不会将解码器在 $t - 1$ 时刻的输出作为输入传输给 $t$ 时刻（我们在模型推理阶段会这么做来建立模型输出的上下文联系）。我们会直接将正确的 ground truth  中的第 $t - 1$ 时刻的内容作为输入传输给 $t$ 时刻。

![IMG_58B92A9252D3-1](https://markdown-img-1304853431.file.myqcloud.com/IMG_58B92A9252D3-1.jpeg)

<center>Fig 7. 训练阶段的 Seq2Seq 模型，梯度沿紫色箭头路线反向传播</center>

在训练阶段中，如果我们选择 SGD 作为优化方法，我们可以这样训练模型：

1. 随机在训练集中选择一个 (input, output) 数据
2. 使用这个数据让模型进行前向计算
3. 随机选择模型在解码阶段的一个输出进行 Loss 计算并进行反向传播
4. 更新模型权重参数

## 5 Seq2Seq 的缺陷与改进方法

Seq2Seq 模型到现在依然是解决非同步非对齐序列预测问题的经典模型之一。然而，它的结构存在一个缺陷 - 这个缺陷会在模型接受长输入序列时被暴露无遗。注意到图3中的编码器-解码器模型。在这个模型中，编码器与解码器只通过隐藏状态（总结向量）连接起来。这直接导致了两个问题：

1. 当输入序列较长时，这个隐藏状态可能无法完全容纳输入中蕴含的所有信息量，从而导致模型接受信息的损失。**【编码器与解码器之间的连接太狭窄，无法传递所有信息】**
2. 如果输入序列较长，一些信息可能会在编码器接受新信息的过程中被“稀释”，这可能会导致解码器遗漏输入中的关键信息。**【编码器中所有的隐藏状态都含有独特的信息，但是只有最终状态被传递给了解码器】**

而这两个问题的解决方案则是目前大有一统 CNN，RNN，多模态学习 之势的 Transformer 模型的底层结构 - 注意力机制（Attention Mechanism）