---
layout: post
tags: [ NLP, Neural Network ]
category: [ Neural Network ]
title: "NLP 101: Transformer Framework"
banner: "/assets/images/banners/NeuralNetworkBackground.jpg"
lang: "ch"
---

## 0 背景知识

1.  [词嵌入 - Word Embedding]({{site.baseurl}}/2021/Word-Embedding.html)

    将自然语言在转化到向量表示的同时保留词汇原有的语义余上下文信息

2.  [编码器-解码器结构 - Encoder-Decoder Architecture]({{site.baseurl}}/2021/Seq2Seq.html) 

    编码器-解码器结构将任务分成两个步骤：先由编码器将输入编码，再通过解码器将编码结果解析成真正的输出

3.  [注意力机制 - Attention Mechanism]({{site.baseurl}}/2022/Attention-Mechanism.html)

    一种通过动态赋予编码器所有隐藏状态权重让解码器充分，有重点的获取信息的方法。

## 1 Transformer 中的注意力机制

### 1.1 Self-Attention 自注意力机制



### 1.2 Multi-Head Attention 多头注意力机制



